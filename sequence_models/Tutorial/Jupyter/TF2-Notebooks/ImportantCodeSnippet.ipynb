{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always stick to the Standard Code after you have loaded your data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5qUXoOLe4C7b"
   },
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eej8zsYi4T-m"
   },
   "source": [
    "**Note:** Make sure you read the **dataset** in the variable dataset! So that you can use the code diretly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdPbfHLc4PXw"
   },
   "source": [
    "dataset = pd.read_csv('weather.csv')\n",
    "dataset.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQuSzQeW6_Ad"
   },
   "source": [
    "# Default Models: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPpW_bEQ7D1t"
   },
   "source": [
    "For classification and Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUN6kkgn699e"
   },
   "source": [
    "features = 9\n",
    "classes = 1\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[features]),\n",
    "    tf.keras.layers.Dense(128, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(classes)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC2oWOdR7dLA"
   },
   "source": [
    "features = 9\n",
    "classes = 1\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=[None, features]),\n",
    "    tf.keras.layers.LSTM(50),\n",
    "    tf.keras.layers.Dense(classes),\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU9ZBITz9w-A"
   },
   "source": [
    "# Defining the Model\n",
    "embedding_dim = 64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(input_dim=VOCABSIZE,\n",
    "                            output_dim=embedding_dim,\n",
    "                            mask_zero=True,\n",
    "                            input_length=MAX_SEQUENCE),\n",
    "  tf.keras.layers.LSTM(50, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(50),\n",
    "  tf.keras.layers.Dense(1)])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmDmyDmX-buA"
   },
   "source": [
    "embedding = 64\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Embedding(input_dim=VOCABSIZE,\n",
    "                                                              output_dim=embedding,\n",
    "                                                              input_length=MAX_SEQUENCE,\n",
    "                                                              mask_zero=True),\n",
    "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(0.5),\n",
    "                                    tf.keras.layers.Dense(5)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "root_path = os.path.join(os.curdir, 'dataset', 'cats_and_dogs')\n",
    "\n",
    "# Let's read the dataset\n",
    "BATCH = 32\n",
    "IMG_HEIGHT = IMG_WIDTH = 224\n",
    "BUFFER = 1000\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Include validation_split, subsetname and seed when needed\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(root_path, 'train'),\n",
    "    batch_size=BATCH,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    ")\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(root_path, 'validation'),\n",
    "    batch_size=BATCH,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    ")\n",
    "\n",
    "train = train.cache().shuffle(BUFFER).prefetch(AUTOTUNE)\n",
    "val = val.cache().prefetch(AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(factor=0.2),\n",
    "    tf.keras.layers.RandomContrast(factor=0.4)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "rescaled = tf.keras.layers.Rescaling(1. /255)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    rescaled,\n",
    "    augmentation,\n",
    "    tf.keras.layers.Conv2D(16, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    tf.keras.layers.Conv2D(16, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=15)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_classifier/', save_best_only=True)\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=50, callbacks=[earlystop, checkpoint])\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 10))\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "# Let's try transfer learning, Always include the image shape as well.\n",
    "basemodel = tf.keras.applications.MobileNetV2(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "basemodel.trainable = False\n",
    "basemodel.summary()\n",
    "\n",
    "preprocess_layer = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "# Note this preprocess layer wants input in range [0,255]. That's why we used rescaling layer above instead of\n",
    "# manually dividing it. But here the preprocess_layer will do that for ya!\n",
    "\n",
    "# First Augment the image and then apply preprocess_layer on it.\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "input_layer = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = augmentation(input_layer)\n",
    "x = preprocess_layer(x)\n",
    "x = basemodel(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "out = tf.keras.layers.Dense(NUM_CLASSES)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=out)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=15)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_classifier/', save_best_only=True)\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=50, callbacks=[earlystop, checkpoint])\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 10))\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJCp5dMY95v1"
   },
   "source": [
    "# Compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXNLcbqY98NO"
   },
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['mse'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbpCeYce7RZE"
   },
   "source": [
    "# Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6L7DEUc7X8M"
   },
   "source": [
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_temp/', save_best_only=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training after reaching 60 percent accuracy\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "\n",
    "    # Check accuracy\n",
    "    if(logs.get('loss') < 0.4): # or if(logs.get('accuracy') > 0.95):\n",
    "\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjMl1tbi4zyN"
   },
   "source": [
    "# Time Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i_VLRaW4uRh"
   },
   "source": [
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset.set_index('date', inplace=True)\n",
    "dataset = dataset.sort_index(ascending=True)\n",
    "\n",
    "# Time Format: 2004-03-10 18:00:00"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO7cAihf496P"
   },
   "source": [
    "# If flow speed and its direction(in degree) is given then\n",
    "wv = dataset.pop('Speed (km/h)')\n",
    "wd = dataset.pop('Direction (degrees)') * np.pi / 180   # Converting to radian\n",
    "\n",
    "dataset['Wx'] = wv * np.cos(wd)\n",
    "dataset['Wy'] = wv * np.sin(wd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If periodic data present\n",
    "\n",
    "timestamp_s = dataset.index.map(pd.Timestamp.timestamp)\n",
    "# Lets include day periodicity, this way model gets access to most important frequency features\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Memory Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lets Reduce the Size of the dataset\n",
    "\n",
    "def Reduce_Me(dataset):\n",
    "    Initial = data.memory_usage().sum()/ 1024**2\n",
    "    print(\"Initial Memory : {:.2f} MB\".format(Initial))\n",
    "    Columns = dataset.columns\n",
    "    for column in Columns:\n",
    "        Dtype = str(data[column].dtype)\n",
    "        \n",
    "        min_ = data[column].min()\n",
    "        max_ = data[column].max()\n",
    "            \n",
    "        if 'int' in Dtype:\n",
    "            if min_ > np.iinfo(np.int8).min and max_ < np.iinfo(np.int8).max:\n",
    "                data[column] = data[column].astype(np.int8)\n",
    "            elif min_ > np.iinfo(np.int16).min and max_ < np.iinfo(np.int16).max:\n",
    "                data[column] = data[column].astype(np.int16)\n",
    "            elif min_ > np.iinfo(np.int32).min and max_ < np.iinfo(np.int32).max:\n",
    "                data[column] = data[column].astype(np.int32)\n",
    "            elif min_ > np.iinfo(np.int64).min and max_ < np.iinfo(np.int64).max:\n",
    "                data[column] = data[column].astype(np.int64)\n",
    "        else:\n",
    "            if min_ > np.finfo(np.float16).min and max_ < np.finfo(np.float16).max:\n",
    "                data[column] = data[column].astype(np.float16)\n",
    "            elif min_ > np.finfo(np.float32).min and max_ < np.finfo(np.float32).max:\n",
    "                data[column] = data[column].astype(np.float32)\n",
    "            elif min_ > np.finfo(np.float64).min and max_ < np.finfo(np.float64).max:\n",
    "                data[column] = data[column].astype(np.float64)\n",
    "    Final = data.memory_usage().sum()/1024**2\n",
    "    print(\"Final Memory : {:.2f} MB\".format(Final))\n",
    "    print(\"Reduced By: {:.2f}%\".format((Initial-Final)/Initial * 100))\n",
    "    return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6JJA6HI5WAX"
   },
   "source": [
    "# Label Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajNHD1UN5NQ-"
   },
   "source": [
    "for column in dataset.select_dtypes('object').columns:\n",
    "  encoder = LabelEncoder()\n",
    "  dataset[column] = encoder.fit_transform(dataset[column])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# In presence of null values\n",
    "\n",
    "for column in dataset.select_dtypes('object').columns:\n",
    "  encoder = LabelEncoder()\n",
    "  null_index = dataset.loc[dataset[column].isnull()].index\n",
    "  dataset[column] = encoder.fit_transform(dataset[column])\n",
    "  dataset.loc[null_index, column] = np.nan\n",
    "  # Note Label encoder also changes np.nan so this will reset it back to np.nan"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "columns = dataset.select_dtypes('object').columns\n",
    "data[columns] = dataset.select_dtypes('object').apply(LabelEncoder().fit_transform)  # One direct way of Encoding \n",
    "# Another Way : OneHotEncoder().fit_transform(df) or LabelEncoder().fit_transform(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwaZCTs55mD1"
   },
   "source": [
    "# Null Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2GhO6AB5YXt"
   },
   "source": [
    "# Lets check the NULL Values first\n",
    "\n",
    "null = pd.DataFrame(dataset.isnull().sum()).rename(columns={0:\"Total\"})\n",
    "null['percentage'] = null['Total'] / len(dataset)\n",
    "null.sort_values('percentage',ascending=False).head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nqt0NP85rkt"
   },
   "source": [
    "# Forward fill, for time series data, if this still has some null values then we will fill it with Median\n",
    "dataset.fillna(method=\"ffill\", inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Still there are some null values lets fill them with the mean value\n",
    "meanvalue = dataset['variable'].mean() # or median()\n",
    "dataset['variable'] = dataset['variable'].fillna(value=meanvalue)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S0q2s778hGH"
   },
   "source": [
    "# Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(dataset['Day Sin'].to_numpy(), label='Day Sin')\n",
    "plt.plot(dataset['Day Cos'].to_numpy(), label='Day Cos')\n",
    "plt.legend()\n",
    "plt.axis()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YydIgRTd6Dde"
   },
   "source": [
    "# For plotting if needed\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for idx, col in enumerate(dataset.columns):\n",
    "    plt.subplot(len(dataset.columns), 1, idx+1)\n",
    "    plt.plot(dataset[col], label=col)\n",
    "    plt.legend()    \n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lets plot how everything looks like\n",
    "\n",
    "# Lets do EDA at Day level, KDE Plot\n",
    "\n",
    "plt.figure(figsize=(25, 25))\n",
    "column = dataset.select_dtypes(\"float\").columns\n",
    "for idx, col in enumerate(column):\n",
    "    ax = plt.subplot(8, 2, idx+1)\n",
    "    sns.kdeplot(dataset[col], ax=ax)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid()\n",
    "    plt.axis()\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# For imbalanced dataset\n",
    "\n",
    "def plot_metrics(history):\n",
    "  plt.figure(figsize=(20,20))\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend();"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kujbRFr6IWs"
   },
   "source": [
    "# Splitting dataset\n",
    "# Lets split the dataset \n",
    "\n",
    "train = dataset[0 : int(0.7 * len(dataset))]\n",
    "val = dataset[int(0.7 * len(dataset)):int(0.9 * len(dataset))]\n",
    "test = dataset[int(0.9 * len(dataset)):]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lets first split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(dataset, target, test_size=0.2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lets split to train and validation set\n",
    "train_size = int(len(X_train) * 0.8)\n",
    "index = tf.random.shuffle(tf.range(len(X_train)))\n",
    "\n",
    "train, train_label = tf.gather(X_train, index[:train_size]), tf.gather(Y_train, index[:train_size])\n",
    "val, val_label = tf.gather(X_train, index[train_size:]), tf.gather(Y_train, index[train_size:])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_val_split(data, target, train_size=0.8):\n",
    "  train_len = int(len(data) * train_size)\n",
    "  index = tf.random.shuffle(tf.range(len(data)))\n",
    "  x_train = tf.gather(data, index[:train_len])\n",
    "  y_train = tf.gather(target, index[:train_len])\n",
    "\n",
    "  x_val = tf.gather(data, index[train_len:])\n",
    "  y_val = tf.gather(target, index[train_len:])\n",
    "\n",
    "  return (x_train, y_train), (x_val, y_val)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train, val = train_val_split(train, labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtIGSwPU6MSM"
   },
   "source": [
    "train_label = train['Target_label']\n",
    "train = train.drop(['Target_label'], axis=1)\n",
    "\n",
    "val_label = val['Target_label']\n",
    "val = val.drop(['Target_label'], axis=1)\n",
    "\n",
    "test_label = test['Target_label']\n",
    "test = test.drop(['Target_label'], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKD0637P6WoU"
   },
   "source": [
    "mean = train.mean()\n",
    "std = train.std() + 1e-12\n",
    "\n",
    "train = (train - mean) / std\n",
    "val = (val - mean) / std\n",
    "test = (test - mean) / std"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mgAq6nV7oqg"
   },
   "source": [
    "# Data Loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B6yYt836cMF"
   },
   "source": [
    "def non_sequential_train_loader(data, labels, batchsize=32, buffersize=100):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "  dataset = dataset.cache().shuffle(buffersize).batch(batchsize)\n",
    "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aawQv7uW6lSs"
   },
   "source": [
    "def non_sequential_val_loader(data, labels, batchsize=32):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "  dataset = dataset.cache().batch(batchsize)\n",
    "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6Mp7kUw6qX9"
   },
   "source": [
    "def non_sequential_test_loader(data, batchsize=32):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "  dataset = dataset.cache().batch(batchsize)\n",
    "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eORjU8t63W9"
   },
   "source": [
    "def create_sequential_train_loader(series, window_size=24, batchsize=32, buffersize=100):\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size+1, drop_remainder=True, shift=1)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1,:-1], window[-1,-1]), num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.cache().shuffle(buffersize).batch(batchsize)\n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cP6MrDrW8Qqs"
   },
   "source": [
    "def create_sequential_val_loader(series, window_size=24, batchsize=32):\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size+1, drop_remainder=True, shift=1)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1,:-1], window[-1,-1]), num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.cache().batch(batchsize)\n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJfxn4dc6v6G"
   },
   "source": [
    "train_load = non_sequential_train_loader(train.to_numpy(), train_label.to_numpy())\n",
    "val_load = non_sequential_val_loader(val.to_numpy(), val_label.to_numpy())\n",
    "test_load = non_sequential_test_loader(test.to_numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dGQGETd7_gN"
   },
   "source": [
    "train = tf.concat([train.to_numpy(), train_label.to_numpy().reshape(-1,1)], axis=1)\n",
    "val = tf.concat([val.to_numpy(), val_label.to_numpy().reshape(-1,1)], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhU20fME8CV1"
   },
   "source": [
    "train = create_sequential_train_loader(train)\n",
    "val = create_sequential_val_loader(val)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh1AmoNW8Cbs"
   },
   "source": [
    "for X, Y in train.take(1):\n",
    "    print(X.shape)\n",
    "    print(Y.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvQ05_YP8t5h"
   },
   "source": [
    "# Natural Language Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKY8m1Um8wNF"
   },
   "source": [
    "def clean_string(dataframe, field, targets=None, is_train=True):\n",
    "\n",
    "  dataframe[field] = dataframe[field].str.replace(\"[{}]\".format(string.punctuation), \" \")\n",
    "\n",
    "  dataframe[field] = dataframe[field].str.lower()\n",
    "\n",
    "  # Everything in one line\n",
    "  lines = []\n",
    "  target = []\n",
    "\n",
    "  for idx, line in enumerate(dataframe[field].to_numpy()):\n",
    "    \n",
    "    if line != '':\n",
    "      lines.append(line.strip())\n",
    "      if targets is not None:\n",
    "        target.append(targets[idx])\n",
    "\n",
    "  if is_train:\n",
    "    return lines, target\n",
    "\n",
    "  return lines"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDqrIbXE-KVn"
   },
   "source": [
    "def clean_string(dataframe, field):\n",
    "  dataframe[field] = dataframe[field].str.replace(\"[{}]\".format(string.punctuation), '')\n",
    "  dataframe[field] = dataframe[field].str.lower()\n",
    "\n",
    "  lines = []\n",
    "  target = []\n",
    "\n",
    "  for idx, text in enumerate(dataframe[field].to_numpy()):\n",
    "\n",
    "    if text != '':\n",
    "      lines.append(text.strip())\n",
    "      target.append(list(dataset.iloc[idx, 'target_value'].to_numpy().astype(np.float32)))\n",
    "  \n",
    "  return lines, target"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"   # EXample\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname='aclImdb_v1',\n",
    "                                  origin=url,\n",
    "                                  cache_dir='.',\n",
    "                                  untar=True,\n",
    "                                  cache_subdir='')   # Will download and load it in the same directory\n",
    "\n",
    "path = os.path.join(os.curdir, 'aclImdb')\n",
    "BATCH = 32\n",
    "SEED = 123\n",
    "\n",
    "# If you need to remove some unwanted directories\n",
    "shutil.rmtree(os.path.join(path, 'train', 'unsup'))\n",
    "\n",
    "\n",
    "train = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(path, 'train'),\n",
    "    subset='training',\n",
    "    validation_split=0.2,\n",
    "    seed=SEED,\n",
    "    batch_size=BATCH\n",
    ")\n",
    "val = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(path, 'train'),\n",
    "    subset='validation',\n",
    "    validation_split=0.2,\n",
    "    seed=SEED,\n",
    "    batch_size=BATCH\n",
    ")\n",
    "test = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(path, 'test'),\n",
    "    batch_size=BATCH\n",
    ")\n",
    "for text, label in train.take(1):\n",
    "    print(text.shape)\n",
    "    print(label.shape)\n",
    "    # print(text)\n",
    "    # print(label)\n",
    "\n",
    "\n",
    "def clean_string(instance):\n",
    "    instance = tf.strings.lower(instance)\n",
    "    instance = tf.strings.regex_replace(instance, '<br />', '')\n",
    "    instance = tf.strings.regex_replace(instance, '[{}]'.format(string.punctuation), '')\n",
    "    instance = tf.strings.strip(instance)\n",
    "    return instance\n",
    "\n",
    "\n",
    "\n",
    "MAX_SEQUENCE = 250\n",
    "VOCABSIZE = 10000\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABSIZE,\n",
    "    standardize=clean_string,   # Note the vectorizer doesnt remove html tags\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE\n",
    ")\n",
    "data = train.map(lambda text, label: text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "vectorizer.adapt(data)\n",
    "\n",
    "\n",
    "def vectorize(text, label):\n",
    "    return vectorizer(text), label\n",
    "\n",
    "\n",
    "train = train.map(vectorize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val = val.map(vectorize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = test.map(vectorize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "for text, label in train.take(1):\n",
    "    print(text.shape)\n",
    "    print(label.shape)\n",
    "    # print(text)\n",
    "    # print(label)\n",
    "\n",
    "train = train.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "val = val.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test = test.cache().prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKiuOx248yaM"
   },
   "source": [
    "labels = train.target.to_numpy()\n",
    "\n",
    "train, labels = clean_string(train, \"text\", targets=labels)\n",
    "test = clean_string(test, \"text\", is_train=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqS1l2nc85_0"
   },
   "source": [
    "MAX_SEQUENCE = 250\n",
    "VOCABSIZE = 10000\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABSIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mV5orGmm9GR_"
   },
   "source": [
    "text = train.map(lambda text, label: text)          # Adapt only using training set.\n",
    "vectorizer.adapt(text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaOekIgK9Lc1"
   },
   "source": [
    "def vectorize_text(text, labels):\n",
    "  return vectorizer(text), labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjBEc5d_9Nje"
   },
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train = train.map(vectorize_text, num_parallel_calls=AUTOTUNE)\n",
    "val = val.map(vectorize_text, num_parallel_calls=AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWm4VEir9PHW"
   },
   "source": [
    "for X, Y in train.take(1):\n",
    "  print(X.shape)\n",
    "  print(Y.shape)\n",
    "  print(X)\n",
    "  print(Y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6NHFz3K9Q1u"
   },
   "source": [
    "BATCH_SIZE = 64\n",
    "train = train.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "val = val.cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7K6IkUE9twa"
   },
   "source": [
    "# Redundancy Removal(Avoid this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LEts look for redundant variables\n",
    "correlation = dataset.corr()\n",
    "upper = correlation.where(np.triu(np.ones(correlation.shape), k=1).astype(bool))\n",
    "redundant = [col for col in upper if np.any(np.abs(upper[col]) >= 0.95)]\n",
    "redundant"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sns.heatmap(correlation.loc[correlation['variable'].abs() >= 0.95, correlation['variable'].abs() >= 0.95],\n",
    "            annot=True, cmap=plt.cm.autumn_r, fmt='0.3f')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Imbalanced dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "METRICS = [\n",
    "           tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc'),\n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "zeros, ones = np.bincount(dataset.target_variable.to_numpy())    # Here Zeros count > ones. Verify to see if all good.\n",
    "# You can verify using value_counts()\n",
    "total = zeros + ones\n",
    "bias = np.log(ones / zeros)\n",
    "\n",
    "w0 = (1 / zeros) * (total / 2.0)\n",
    "w1 = (1 / ones) * (total / 2.0)\n",
    "\n",
    "class_weights = {0 : w0, 1: w1}\n",
    "class_weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_model(output_bias=None):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "  model = tf.keras.models.Sequential([\n",
    "                                      tf.keras.layers.Flatten(input_shape=[24]),\n",
    "                                      tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "                                      tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n",
    "  ])\n",
    "  return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model = create_model(bias)                  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=METRICS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "history = model.fit(train_load, validation_data=val_load, epochs=100, class_weight=class_weights, callbacks=[earlystop, checkpoint])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def forecast(model, series, batch_size=32):\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.cache().batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return model.predict(dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lets try non sequential loader\n",
    "def forecasting(model,series, window_size=24, batchsize=32):\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size, drop_remainder=True, shift=1)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size))\n",
    "    dataset = dataset.cache().batch(batchsize).prefetch(AUTOTUNE)\n",
    "    return model.predict(dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "window_size = 60\n",
    "prediction = forecasting(model, test_data.to_numpy(), window_size=window_size)\n",
    "prediction = np.where(prediction > 0.5, 1, 0)\n",
    "prediction.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "target = test_label.to_numpy()[window_size-1:]   # Because all the previous values will be dropped!\n",
    "target.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tf.keras.metrics.Accuracy()(target, tf.squeeze(prediction, axis=-1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Sequential Model Loss: {tf.keras.metrics.mean_squared_error(tf.squeeze(prediction, axis=-1), test_label.to_numpy())}')\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ImportantCodeSnippet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
