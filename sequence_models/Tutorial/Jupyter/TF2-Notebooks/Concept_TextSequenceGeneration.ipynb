{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzcKvIf9DtaJ"
   },
   "source": [
    "Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_ywLz4WD8xn"
   },
   "source": [
    "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
    "\n",
    "1. The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
    "\n",
    "2. The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
    "\n",
    "3. As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hv2sFwetDnGq"
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cn6TjcKhEDZX",
    "outputId": "b31173ff-5c7d-47fd-a701-ef13dcc2568b"
   },
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-nBasDa9EGAH",
    "outputId": "daf29c83-f3c9-42d6-a335-3d4eb98c89e7"
   },
   "source": [
    "path_to_file"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X2Nq2c5EIlg",
    "outputId": "f169504a-2d12-48c3-8cd1-55d7eddc42e0"
   },
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')  # without decode we will have b'First...'\n",
    "# Always make sure that your strings are in the right format!\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_V3edzCER38",
    "outputId": "d7158a89-8550-4f4c-ef0c-730551ba8988"
   },
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmOaarYhEggT",
    "outputId": "02cc29a8-73aa-47b5-fd47-0c63701d80e2"
   },
   "source": [
    "# Lets create vocabulary, which is the the unique characters in the set\n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQQpvYvEExh9",
    "outputId": "7f491b0c-4b68-4e79-c25e-090eedaf5d92"
   },
   "source": [
    "vocab[:5]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ9N-hvTFSKS"
   },
   "source": [
    "**Process the text**\n",
    "Vectorize the text\n",
    "Before training, you need to convert the strings to a numerical representation.\n",
    "\n",
    "The **tf.keras.layers.StringLookup** layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8V1bq1TIE2DV",
    "outputId": "ba0cb34b-98ad-470f-a6d7-5473c073e134"
   },
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iwRspPNvFari"
   },
   "source": [
    "# Luckily we already did that.\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu2gCpEQGkdB"
   },
   "source": [
    "It converts from tokens to character IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GtvaG9nGcOe",
    "outputId": "d245ba13-7541-4fed-b3bd-0073b098ecbd"
   },
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1yD2R84Grx5"
   },
   "source": [
    "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use **tf.keras.layers.StringLookup(..., invert=True)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2yhaHp-HIcP"
   },
   "source": [
    "Note: Here instead of passing the original vocabulary generated with **sorted(set(text))** use the **get_vocabulary()** method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1F3dm2xGfy_",
    "outputId": "d08209d5-56f2-455f-b2b7-02057a88d868"
   },
   "source": [
    "# Example\n",
    "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
    "layer = tf.keras.layers.StringLookup()\n",
    "layer.adapt(data)\n",
    "layer.get_vocabulary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcaS7ANoHPSf",
    "outputId": "fa6c80ef-eb17-4234-c992-d64dfd3bec08"
   },
   "source": [
    "layer(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4Zn4r6peHRjV"
   },
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yocOQuCQISik",
    "outputId": "5f7b7329-fef5-4497-9927-6ab25cee7351"
   },
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0RP6jzKIj2C"
   },
   "source": [
    "You can **tf.strings.reduce_join** to join the characters back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4RCSbUCIUr0",
    "outputId": "28cb49ae-c733-4d9c-aaa6-0b4020c2fccf"
   },
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6n-DQTtJIm3G"
   },
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVQhL9m7I2Zf"
   },
   "source": [
    "**The prediction task**\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM5qwkoUJEjn"
   },
   "source": [
    "**Create training examples and targets**\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXtt-h_rJT73"
   },
   "source": [
    "To do this first use the **tf.data.Dataset.from_tensor_slices** function to convert the text vector into a stream of character indices.\n",
    "\n",
    "Note: Newline character \\n is also considered as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j03kTW0HJbl7",
    "outputId": "eb07bd24-b7fb-40ee-fe05-d8d127a422e2"
   },
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tPNh8dYRJ0eh"
   },
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLA4MhP_J4t1",
    "outputId": "427089aa-6bb6-4394-dfa6-1123806342ba"
   },
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km8JaZFIKAMx"
   },
   "source": [
    "The **batch** method lets you easily convert these individual characters to sequences of the desired size.\n",
    "\n",
    "**Note:** If your program requires data to have a statically known shape (e.g., when using XLA), you should use drop_remainder=True. Without **drop_remainder=True** the shape of the output dataset will have an unknown leading dimension due to the possibility of a smaller final batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5K50PPk8J6hH",
    "outputId": "176f9e3b-97ad-4002-bce5-0f625d53e903"
   },
   "source": [
    "seq_length = 100\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpA6OO2PKQkz",
    "outputId": "f6713f14-4d31-4d90-a94a-55176d39349f"
   },
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOEATp_5Kd2B"
   },
   "source": [
    "For training you'll need a dataset of **(input, label)** pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
    "\n",
    "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cIXWINTDKWl2"
   },
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhCJ7v6lKhn1",
    "outputId": "05d872a3-2be7-481e-bd2f-35e2f5c59d68"
   },
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7rufG2WK2wy"
   },
   "source": [
    "Note: Here the dataset contains number of batches so map function will treat whole first batch as a single instance, second batch as second instance and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Z52Gtq1ZKi0p"
   },
   "source": [
    "dataset = sequences.map(split_input_target)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0j7oBtsSLDwR",
    "outputId": "a7954b90-8a7c-48ab-e53e-88a82bfc7bfa"
   },
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-LcGjO7NWo7",
    "outputId": "5ea4f9ce-1d99-4e34-b1d0-a74c977c4f66"
   },
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9wfr6c6NcPx",
    "outputId": "3ff53ef4-f953-45f9-d789-52414d2c80b7"
   },
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLqHcvHue0HO"
   },
   "source": [
    "We could have also implemented it using the tf.data.Window function like we did in timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Rjevt4LIQ2"
   },
   "source": [
    "Note this we could have also acheived using window() function as in \n",
    "time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sT8HKWLeLGFq"
   },
   "source": [
    "def create_sequence_sequence_batch(series, window_size=1, buffer_size=1000):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size+1, shift=window_size+1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
    "  \n",
    "  return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SNbTRlZ7MPwQ"
   },
   "source": [
    "loader = create_sequence_sequence_batch(all_ids, window_size=seq_length)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cGIZEZlMWIi",
    "outputId": "e2780d1e-a2db-4ead-8390-542bd602c222"
   },
   "source": [
    "for input_example, target_example in loader.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHclHwqceEtA",
    "outputId": "c90493b2-9041-4486-e3b7-ffddf4955040"
   },
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "loader = (\n",
    "    loader\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "loader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0MYmz1NeRHS",
    "outputId": "e048f18c-18a0-401a-830d-c869cc23cec9"
   },
   "source": [
    "for input_example, target_example in loader.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "849DGX1ZMfCU"
   },
   "source": [
    "def final_create_sequence_sequence_batch(series, window_size=1, batch_size=64,buffer_size=10000):\n",
    "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size+1, shift=window_size+1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1], window[1:]), num_parallel_calls=AUTOTUNE)\n",
    "  dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "  dataset = dataset.prefetch(AUTOTUNE)\n",
    "  return dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "boc1092TNAmb"
   },
   "source": [
    "dataset = final_create_sequence_sequence_batch(all_ids, window_size=seq_length)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wafBR4_GNGM8",
    "outputId": "1b751916-35e2-43bc-a198-3ddd296225b5"
   },
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTMQXZ_QPPr4"
   },
   "source": [
    "Defining the **Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gHPbGxcDNI_l"
   },
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UnoVv9l4PV3-"
   },
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "C35fKNB6PlIy"
   },
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMFYyJeVPt-x"
   },
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mFnpDSpPowL",
    "outputId": "0fd7cd41-84a4-41ea-d7b2-07d8cbf53743"
   },
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOPmxvXOP3Zb",
    "outputId": "173d22b2-cba7-45e5-908e-22296841ea39"
   },
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hrvX7gUfQCEZ"
   },
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "g_m17qcWQcBF"
   },
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otnHGFRvQeuX",
    "outputId": "081fe2cc-57d6-4279-c0a9-e93258b746b9"
   },
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "USr8mjXgQhxG"
   },
   "source": [
    "skip_ids = ids_from_chars(['[UNK]'])[:, None]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hPPU8aMJVvtz"
   },
   "source": [
    "sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJxiOd_yV8B_",
    "outputId": "d9a593da-20b7-4928-9f91-8c04ccba98c9"
   },
   "source": [
    "sparse_mask"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "XzWpXB8vV9hh"
   },
   "source": [
    "prediction_mask = tf.sparse.to_dense(sparse_mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2Dxlu4CWBPU",
    "outputId": "f7dc7444-c16e-4e53-99d7-44c9d22cbe2e"
   },
   "source": [
    "prediction_mask   # -inf is at the [UNK] place, so that it doesnt get printed."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "dkPK7_mrWC4Y"
   },
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction. Because its like sequence to sequence so we\n",
    "    # only care about the last character\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    # The dimension of the logits provides the number of labels and it \n",
    "    # generated those labels using the predicted_logits.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc5d4QgbYMad",
    "outputId": "9a8236df-2871-4a29-a358-beaee06fe2da"
   },
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qKEBfKLYPw0"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Concept: TextSequenceGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
