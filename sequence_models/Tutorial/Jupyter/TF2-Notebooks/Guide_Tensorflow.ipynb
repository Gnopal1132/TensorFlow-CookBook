{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b6c1f1",
   "metadata": {},
   "source": [
    "# Question to practise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9838d",
   "metadata": {},
   "source": [
    "# Part 1: Tensorflow and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dd2af",
   "metadata": {},
   "source": [
    "1. Plot the Activation function and their derivatives of Relu,Tanh and Sigmoid. Which one of them are Zero Centered?\n",
    "2. What is Keras and Tensorflow. Why are they used ?\n",
    "3. Build a simple image classifier using fashion_mnist. Plot the dataset, do hyperparameter tuning, tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24585be5",
   "metadata": {},
   "source": [
    "4. Implement the following model in keras and tensorflow on fetch_california_dataset.Plot the dataset, do hyperparameter tuning, tensorboard. Implement the earlystopping, and modelcheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e1dd483d",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "Image('img1.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a1f39c",
   "metadata": {},
   "source": [
    "5. Is it okay to initialize all the weights to the same value as long as that value is\n",
    "selected randomly using He initialization?\n",
    "\n",
    "6. Define different type of activation functions and their properties?\n",
    "7. Define different type of initializers ?\n",
    "8. What is BN? How does it differs in train and test time ?\n",
    "9. Does dropout slow down training? Does it slow down inference (i.e., making\n",
    "predictions on new instances)? What are about MC dropout?\n",
    "\n",
    "\n",
    "10.Code Following:\n",
    "\n",
    "    a. Build a DNN with five hidden layers of 100 neurons each, He initialization,\n",
    "    and the ELU activation function.\n",
    "    \n",
    "    b. Using Adam optimization and early stopping, try training it on MNIST but\n",
    "    only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
    "    next exercise. You will need a softmax output layer with five neurons, and as\n",
    "    always make sure to save checkpoints at regular intervals and save the final\n",
    "    model so you can reuse it later.\n",
    "    \n",
    "    c. Tune the hyperparameters using cross-validation and see what precision you\n",
    "    can achieve.\n",
    "    \n",
    "    d. Now try adding Batch Normalization and compare the learning curves: is it\n",
    "    converging faster than before? Does it produce a better model?\n",
    "    \n",
    "    e. Is the model overfitting the training set? Try adding dropout to every layer\n",
    "    and try again. Does it help?\n",
    "    \n",
    "11. Transfer learning.\n",
    "    a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "    model, freezes them, and replaces the softmax output layer with a new one.\n",
    "    \n",
    "    b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\n",
    "    how long it takes. Despite this small number of examples, can you achieve\n",
    "    high precision?\n",
    "    \n",
    "    c. Try caching the frozen layers, and train the model again: how much faster is it\n",
    "    now?\n",
    "    \n",
    "    d. Try again reusing just four hidden layers instead of five. Can you achieve a\n",
    "    higher precision?\n",
    "    \n",
    "    e. Now unfreeze the top two hidden layers.\n",
    "    \n",
    "12. Explain Tensorflow Architecture\n",
    "13. What are different execution modes of tensorflow ?\n",
    "14. What is tf.einsum() ?\n",
    "15. Define a custom Huber loss function with specified threshold. \n",
    "Make sure threshold gets saved after execution to the disk and also show how to reload it.\n",
    "16. Design L1 Regularization with a given factor. Dont use the L1 Regularization directly!\n",
    "17. Implement running Huber Metric.\n",
    "18. Implement a simple Dense layer function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0c1ba",
   "metadata": {},
   "source": [
    "19. Implement the following Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50d6deb",
   "metadata": {},
   "source": [
    "Image('img2.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c077583b",
   "metadata": {},
   "source": [
    "20. Implement jacobian and Hessian matrix using Gradient Tape.\n",
    "21. Design a custom training loop on your favorite dataset.\n",
    "22.Implement OneCycle, Exponential, and Power Scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910b5c1",
   "metadata": {},
   "source": [
    "23. Design the Dataloader for your favorite dataset using Tensorflow Data API and Keras Sequence.\n",
    "It should resize the image having 50x50 and use DataAugmentation.\n",
    "24. What is tfrecord and Protobuf. (Study Chapter 13 Again!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435f054",
   "metadata": {},
   "source": [
    "25. Implement CNN on Fashion_MNIST dataset.\n",
    "Do all the preprocessing and visualization stuff. Try to get as high accuracy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eca97c",
   "metadata": {},
   "source": [
    "26. Implement ResNet Architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8099f387",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Image('Img3.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e7cb5f35",
   "metadata": {},
   "source": [
    "27. Explain all differnet Architectures. Their plus and negative points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e3e1e",
   "metadata": {},
   "source": [
    "28. Implement FullyConvolutional Model from Scratch on your favorite dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0662f",
   "metadata": {},
   "source": [
    "29. Explain Transposed Convolution and the formula for calculating the output size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcdee8",
   "metadata": {},
   "source": [
    "30. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
    "a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\n",
    "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
    "images of 200 × 300 pixels. What is the total number of parameters in the CNN?\n",
    "If we are using 32-bit floats, at least how much RAM will this network require\n",
    "when making a prediction for a single instance? What about when training on a\n",
    "mini-batch of 50 images?\n",
    "\n",
    "31. If your GPU runs out of memory while training a CNN, what are five things you\n",
    "could try to solve the problem?\n",
    "\n",
    "32. Difference between BN and LN. How to implement LayerNormalization used and implemented in Tensorflow?\n",
    "33. Why would you want to add a max pooling layer rather than a convolutional\n",
    "layer with the same stride?\n",
    "34. Can you name the main innovations in AlexNet, compared to LeNet-5? What\n",
    "about the main innovations in GoogLeNet, ResNet, SENet and Xception?\n",
    "35. What is a Fully Convolutional Network? How can you convert a dense layer into\n",
    "a convolutional layer?\n",
    "36. If your GPU runs out of memory while training a CNN, what are five things you\n",
    "could try to solve the problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c10f24",
   "metadata": {},
   "source": [
    "37. Implement Transformer architecture for MachineTranslation English to German from Scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d8edba",
   "metadata": {},
   "source": [
    "Image('Img4.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "930044b2",
   "metadata": {},
   "source": [
    "# Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebac94c",
   "metadata": {},
   "source": [
    "# Importing Libraries and files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed94a732",
   "metadata": {},
   "source": [
    "X = tf.expand_dims(tf.reshape(tf.range(10),shape=(1,5,2)),axis=0)\n",
    "X\n",
    "tf.tile(X,[4,4,1,1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d832d621",
   "metadata": {},
   "source": [
    "def Sigmoid(x,derivative=False):                     # Not Zero Centerd\n",
    "    value = 1/(1 + np.exp(-1.0 * x))\n",
    "    if derivative:\n",
    "        return value*(1-value)\n",
    "    return value\n",
    "\n",
    "def tanh(x, derivative=False):                                #Zero Centered\n",
    "    value = Sigmoid(2.0 * x)\n",
    "    if derivative:\n",
    "        return 4*value*(1-value)\n",
    "    return 2*value - 1\n",
    "\n",
    "def ReLU(x, derivative=False):                         #Not Zero Centered\n",
    "    if derivative:\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        elif x < 0:\n",
    "            return 0\n",
    "        return np.nan\n",
    "    return np.maximum(0,x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ceecd4",
   "metadata": {},
   "source": [
    "X = np.linspace(-20, 20,1000)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)          # Creating a subplot within a figure\n",
    "plt.title('Activation Function')\n",
    "plt.plot(X, Sigmoid(X),color='blue',label='Sigmoid')\n",
    "plt.plot(X, tanh(X),color='red',label='tanh')\n",
    "plt.plot(X, ReLU(X),color='black',label='relu')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Derivative of Activation function')\n",
    "plt.plot(X, Sigmoid(X,derivative=True),color='blue',label='Sigmoid')\n",
    "plt.plot(X, tanh(X,derivative=True),color='red',label='tanh')\n",
    "plt.plot(X, [ReLU(p,derivative=True) for p in X],color='black',label='relu')\n",
    "plt.legend(loc='upper right')"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "8f3c6251",
   "metadata": {},
   "source": [
    "2. Keras is a high level deep learning API that allows to easily build,compile,evaluate and execute all sorts of network. Such\n",
    "API uses some computational backend to execute heavy computations done by the network. tf.keras uses Tensorflow as the \n",
    "computational backend.\n",
    "\n",
    "Numpy is a linear algebra library for python, and one of the most important and popular libraries in Data Science.\n",
    "TensorFlow is a reimplementation of the Numpy API and can be accessed as tf.\n",
    "Last but not least, TensorFlow is sensitive highly about datatypes used. They cannot be implicitly mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5561dfe",
   "metadata": {},
   "source": [
    "# Implementing the Sequential model on fashion mnist\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train,Y_train),(X_test,Y_test) = fashion_mnist.load_data()\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X_train,Y_train,test_size=0.2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cec2cc4",
   "metadata": {},
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ec449ca",
   "metadata": {},
   "source": [
    "# Lets plot the dataset\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 5\n",
    "plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        idx = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, idx+1)\n",
    "        plt.imshow(X_train[idx], cmap ='binary', interpolation='nearest')\n",
    "        plt.title(class_names[Y_train[idx]])\n",
    "        plt.axis('off')\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae778651",
   "metadata": {},
   "source": [
    "# Lets create our model\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(300, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(100, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e2fef4b",
   "metadata": {},
   "source": [
    "model.layers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e421f5e3",
   "metadata": {},
   "source": [
    "W, B = model.layers[1].get_weights()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2356cb15",
   "metadata": {},
   "source": [
    "W"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e76fb9e",
   "metadata": {},
   "source": [
    "B"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc2160d",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c89291c",
   "metadata": {},
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True,show_layer_names=True,show_shapes=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a33c961b",
   "metadata": {},
   "source": [
    "# If the labels are one hot encoded we use categorical cross entropy and if labels are like numbers we use sparse crossentropy.\n",
    "model.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3), metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "230f4d92",
   "metadata": {},
   "source": [
    "history = model.fit(x_train, y_train,validation_data=(x_valid,y_valid), epochs=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c73785",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "plt.figure(figsize=(10,8))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.gca().set_ylim(0,1.0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "113da88c",
   "metadata": {},
   "source": [
    "# Lets do hyperparameter tuning\n",
    "def build_model(n_hidden = 1,neurons=1,learning_rate=1e-3):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
    "    model.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate), metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6132ce91",
   "metadata": {},
   "source": [
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "10020cb0",
   "metadata": {},
   "source": [
    "If we do:\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "\n",
    "The KerasClassifier object is a thin wrapper around the Keras model built using\n",
    "build_model(). Since we did not specify any hyperparameter when creating it, it will\n",
    "just use the default hyperparameters we defined in build_model(). Now we can use\n",
    "this object like a regular Scikit-Learn classifier: we can train it using its fit()\n",
    "method, then evaluate it using its score() method, and use it to make predictions\n",
    "using its predict() method. Note that any extra parameter you pass to the fit()\n",
    "method will simply get passed to the underlying Keras model.\n",
    "\n",
    "\n",
    "\n",
    "What is backend?\n",
    "\n",
    "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
    "\n",
    "At this time, Keras has two backend implementations available: the TensorFlow backend and the Theano backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "012203b4",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {'n_hidden':[1,2,3,4,5],\n",
    "         'neurons':[100,200,300,400,500],\n",
    "         'learning_rate':np.linspace(0, 1,100)}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28bcf04c",
   "metadata": {},
   "source": [
    "In every epoch 3 fold cross validation will be done. and for three fold cross validation 10 configurations will be chosen.\n",
    "Hence total number of times loop will exectute = #epochs  * (#cv * #n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c727e60",
   "metadata": {},
   "source": [
    "search = RandomizedSearchCV(model, params, cv=3, n_jobs=-1, n_iter = 10, random_state=42)\n",
    "search.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=1,callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "search.best_params_\n",
    "search.best_score_\n",
    "final_model = search.best_estimator_.model  # It will give trained keras model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a854a",
   "metadata": {},
   "source": [
    "def give_id(path):\n",
    "    import time\n",
    "    id_ = time.strftime('run_%Y_%m_%d_%H_%M_%S')\n",
    "    return os.path.join(path,id_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "adb609ba",
   "metadata": {},
   "source": [
    "For checkpoints, if file_path = os.path.join(os.curdir or anypath, 'checkpoint.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(file_path,save_best_only=True,monitor='val_loss')\n",
    "\n",
    "model.fit(...)\n",
    "\n",
    "# To load the weights back\n",
    "model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecab85f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Lets build some callbacks\n",
    "callback = []\n",
    "# To save only the best model and only the weights\n",
    "file_path_best = os.path.join(os.curdir,'checkpoint','my_model_best.h5')\n",
    "model_checkpoint_best = tf.keras.callbacks.ModelCheckpoint(filepath=file_path_best, save_best_only=True,save_weights_only=True)\n",
    "callback.append(model_checkpoint_best)\n",
    "# To save the last model\n",
    "file_path_last = os.path.join(os.curdir,'checkpoint','my_model_last.h5')\n",
    "model_checkpoint_last = tf.keras.callbacks.ModelCheckpoint(filepath=file_path_last, save_best_only=False,save_weights_only=True)\n",
    "callback.append(model_checkpoint_last)\n",
    "# Earlystopping with patience 10 and after that it automatically restores the best weights\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "callback.append(earlystop)\n",
    "\n",
    "runtime_id = give_id(os.curdir)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(runtime_id)\n",
    "callback.append(tensorboard)\n",
    "\n",
    "search.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=100,callbacks=callback)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f87c783",
   "metadata": {},
   "source": [
    "# Part 4 Implementing Functional API\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "x_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)      # Its ok to scale testing set no problem "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a102adc9",
   "metadata": {},
   "source": [
    "# Getting train and validation data\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X_train, Y_train, test_size=0.2,random_state=20)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24725073",
   "metadata": {},
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input_A = tf.keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = tf.keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([input_A, hidden2])\n",
    "output = tf.keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = tf.keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = tf.keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592e12c8",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b84b11d9",
   "metadata": {},
   "source": [
    "P"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "191d1587",
   "metadata": {},
   "source": [
    "Q"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "039369de",
   "metadata": {},
   "source": [
    "X = tf.constant([[1,2,3,4,5]])\n",
    "Y = tf.constant([[5,6,7,8,9]])\n",
    "tf.keras.layers.concatenate([X,Y])\n",
    "\n",
    "P = np.arange(10).reshape(1,5,2)\n",
    "Q = np.arange(10,20).reshape(1,5,2)\n",
    "tf.keras.layers.concatenate([P,Q],axis=-1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66806089",
   "metadata": {},
   "source": [
    "model.layers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b6ff015",
   "metadata": {},
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_layer_names=True, show_shapes=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "655e0779",
   "metadata": {},
   "source": [
    "x_input_a, x_input_b = x_train[:,:5],x_train[:,2:]\n",
    "x_valid_a, x_valid_b = x_valid[:,:5],x_valid[:,2:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268ae7a8",
   "metadata": {},
   "source": [
    "x_input_a.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52abb86f",
   "metadata": {},
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bb3f38",
   "metadata": {},
   "source": [
    "model.fit({'wide_input': x_input_a, 'deep_input': x_input_b}, (y_train, y_train),validation_data=((x_valid_a, x_valid_b),(y_valid, y_valid)), epochs=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7cc25",
   "metadata": {},
   "source": [
    "# First Implement callbacks\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.curdir,'checkpoint','my_model.h5')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=file_path, save_best_only=True, save_weights_only=True)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensorboard_id = give_id(os.curdir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(tensorboard_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92956841",
   "metadata": {},
   "source": [
    "# Lets do hyper parameter optimization for above model\n",
    "\n",
    "def build_model(neurons=10,learning_rate=1e-2, n_layers=1, layers=1):\n",
    "    input_a = tf.keras.layers.Input(shape=[5], name='input_a')\n",
    "    input_b = tf.keras.layers.Input(shape=[6], name='input_b')\n",
    "    \n",
    "    hidden1 = tf.keras.layers.Dense(neurons, activation='relu', name='hidden1')(input_b)\n",
    "    hidden2 = tf.keras.layers.Dense(neurons, activation='relu', name='hidden2')(hidden1)\n",
    "    aux_out = tf.keras.layers.Dense(1, name='auxillary_output')\n",
    "    concat = tf.keras.layers.concatenate([input_a, hidden2], name='concatenation_layer')\n",
    "    output = tf.keras.layers.Dense(1, name='main_output')\n",
    "    model = tf.keras.models.Model(inputs=[input_a, input_b], outputs=[output, aux_out])\n",
    "    \n",
    "    model.compile(loss = ['mse', 'mse'], optimizer=tf.keras.optimizers.SGD(learning_rate))\n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbef13f",
   "metadata": {},
   "source": [
    "regressor = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=build_model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087179f",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'neurons': np.array([10,20,30,40,50]),\n",
    "    'learning_rate': np.linspace(0,1,5),\n",
    "    'n_layers': np.arange(5),\n",
    "    'layers':[1,2,3,4,5]\n",
    "}\n",
    "\n",
    "model_cv = RandomizedSearchCV(regressor, params, cv=3, n_iter=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7b25b",
   "metadata": {},
   "source": [
    "history = model_cv.fit([x_input_a, x_input_b], [y_train, y_train],validation_data=([x_valid_a, x_valid_b],[y_valid, y_valid]), epochs=20)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b746ab",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1bb36d6",
   "metadata": {},
   "source": [
    "# Loading MNIST Digit\n",
    "\n",
    "mnist_data = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (x_test, y_test) = mnist_data.load_data()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train,Y_train,test_size=0.2,random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de2fe06",
   "metadata": {},
   "source": [
    "# Part 10 Implement DNN\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=x_train.shape[1:]))\n",
    "for _ in range(5):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=tf.keras.activations.elu, kernel_initializer=tf.keras.initializers.he_normal))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(5, activation=tf.keras.activations.softmax))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "840f9406",
   "metadata": {},
   "source": [
    "mask = np.where(y_train < 5)\n",
    "mask_valid = np.where(y_valid < 5)\n",
    "\n",
    "x_train_a, y_train_a = x_train[mask], y_train[mask]\n",
    "x_valid_a, y_valid_a = x_valid[mask_valid], y_valid[mask_valid]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ba4cba",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93f9631e",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "callbacks = []\n",
    "file_path = os.path.join(os.curdir,'checkpoint','best_model.h5')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=file_path, save_best_only=True, save_weights_only=True)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "callbacks.append(model_checkpoint)\n",
    "callbacks.append(early_stop)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773dfa8d",
   "metadata": {},
   "source": [
    "model.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3e5316",
   "metadata": {},
   "source": [
    "model.fit(x_train_a, y_train_a, validation_data=(x_valid_a, y_valid_a), epochs=30,callbacks=callbacks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe3d1bc",
   "metadata": {},
   "source": [
    " # Lets try to tune it using cross validation\n",
    "\n",
    "def build_model(neurons = 10, layers = 1, learning_rate = 1e-2, input_shape = [28,28]):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape = input_shape))\n",
    "    for _ in range(layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = tf.keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'],\n",
    "                 optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.99))\n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d58b33e",
   "metadata": {},
   "source": [
    "Remember one thing is the param space has 3 keys you cant specifiy cv more than 3. The thing is just put one attribute\n",
    "in the build_model() function which you wont tune, just use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df039667",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)\n",
    "params = {\n",
    "    'neurons' : np.array([50,100,150,200,300]),\n",
    "    'layers' : np.array([1, 2, 3, 4, 5]),\n",
    "    'learning_rate' : np.linspace(0, 1, 5)\n",
    "}\n",
    "\n",
    "search_model = RandomizedSearchCV(model, params, cv=3, n_iter=10, random_state=24)\n",
    "search_model.fit(x_train_a, y_train_a, validation_data=(x_valid_a, y_valid_a), epochs=30,callbacks=callbacks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "685d29f3",
   "metadata": {},
   "source": [
    "Understand how many times loop will execute ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80edbca5",
   "metadata": {},
   "source": [
    "search_model.best_params_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a11426aa",
   "metadata": {},
   "source": [
    "search_model.best_score_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88deb5d7",
   "metadata": {},
   "source": [
    "search_model.best_estimator_.model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1db12e2c",
   "metadata": {},
   "source": [
    "search_model.score(x_valid_a, y_valid_a)  # In score you just pass x_val,y_val"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09a2a5d4",
   "metadata": {},
   "source": [
    "search_model.predict(x_valid_a)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6c9947",
   "metadata": {},
   "source": [
    "# Using Batch Normalization\n",
    "   \n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for _ in range(5):\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer = tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be8e6d58",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a16c881c",
   "metadata": {},
   "source": [
    "model.fit(x_train_a, y_train_a, validation_data=(x_valid_a, y_valid_a), epochs=30,callbacks=callbacks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29479ac3",
   "metadata": {},
   "source": [
    "# Lets try dropout to reduce overftting\n",
    "# Using Batch Normalization\n",
    "   \n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "for _ in range(5):\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer = tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfcac467",
   "metadata": {},
   "source": [
    "model.fit(x_train_a, y_train_a, validation_data=(x_valid_a, y_valid_a), epochs=30,callbacks=callbacks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d618e9f3",
   "metadata": {},
   "source": [
    "Note that model and model_b now share some layers. When you train\n",
    "model_b, it will also affect model. If you want to avoid that, you need to clone\n",
    "model before you reuse its layers. To do this, you must clone model's architecture,\n",
    "then copy its weights (since clone_model() does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ded732d",
   "metadata": {},
   "source": [
    "# Part 5 Transfer learning\n",
    "model_back = tf.keras.models.clone_model(model)\n",
    "model_back.set_weights(model.get_weights())  # Just for backup\n",
    "\n",
    "model_b = tf.keras.models.Sequential(model.layers[:-1])\n",
    "model_b.add(tf.keras.layers.Dense(5, activation=tf.keras.activations.softmax))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e7cd00",
   "metadata": {},
   "source": [
    "for layers in model_b.layers[:-1]:\n",
    "    layers.trainable = False\n",
    "    \n",
    "model_b.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d89c90d",
   "metadata": {},
   "source": [
    "model_b.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4e39abc5",
   "metadata": {},
   "source": [
    "Note: When splitting the dataset. Cross entropy works if labels start with 0 so make the necesary arrangement. In below cases\n",
    "    if you just select starting with 5 i.e. 5,6,7,8,9. It will throw error. make them as 0,1,2,3,4 and later while predicting\n",
    "    make them to correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e29c1c0b",
   "metadata": {},
   "source": [
    "def give_sample(data, labels):\n",
    "    modified_data = []\n",
    "    modified_labels = []\n",
    "    mask = np.where(labels >= 5)\n",
    "    data = data[mask]\n",
    "    labels = labels[mask] \n",
    "    \n",
    "    for id_ in np.unique(labels):\n",
    "        mask = np.where(labels == id_)\n",
    "        modified_data.extend(list(data[mask])[:100])\n",
    "        modified_labels.extend([id_] * 100)\n",
    "        \n",
    "    return np.array(modified_data, dtype=np.uint8),np.array(modified_labels, dtype = np.uint8) - 5  # To bring in range [0,5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fafef15",
   "metadata": {},
   "source": [
    "x_train_new,y_train_new = give_sample(x_train, y_train)\n",
    "x_valid_new,y_valid_new = give_sample(x_valid, y_valid)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "944259cd",
   "metadata": {},
   "source": [
    "model_b.fit(x_train_new,y_train_new,validation_data=(x_valid_new, y_valid_new), shuffle=True, epochs=500,\n",
    "           callbacks=[tf.keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c8735be",
   "metadata": {},
   "source": [
    "for layers in model_b.layers[:-1]:\n",
    "    layers.trainable = True\n",
    "    \n",
    "model_b.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11525a7c",
   "metadata": {},
   "source": [
    "model_b.fit(x_train_new,y_train_new,validation_data=(x_valid_new, y_valid_new), shuffle=True, epochs=500,\n",
    "           callbacks=[tf.keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d5555fa",
   "metadata": {},
   "source": [
    "model_b.save('model_b.h5')\n",
    "\n",
    "# To load the model\n",
    "#tf.keras.models.load_model(PATH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4d75bbd5",
   "metadata": {},
   "source": [
    "# Tf.einsum():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db56ac",
   "metadata": {},
   "source": [
    " Sums the product of the elements of the input operands along dimensions specified using a notation \n",
    "    based on the Einstein summation convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21380b7b",
   "metadata": {},
   "source": [
    "m1 = tf.reshape(tf.range(20),[5,4])\n",
    "m2 = tf.reshape(tf.range(20,40),[4,5])\n",
    "m = tf.reshape(tf.range(25),[5,5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2af131ce",
   "metadata": {},
   "source": [
    "tf.einsum('ij,jk -> ik',m1,m2)         # Dot product"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb2252e8",
   "metadata": {},
   "source": [
    "tf.einsum('ii',m)         # Trace but it must be square matrix"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0f8ab96",
   "metadata": {},
   "source": [
    "tf.einsum('ii -> i',m)    # Diagnol element"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bd284be",
   "metadata": {},
   "source": [
    "tf.einsum('ij -> ji',m)    # Transpose"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df2a6969",
   "metadata": {},
   "source": [
    "b1 = tf.random.normal(shape=[7,5,3])  # Batch, Spatial Dimension\n",
    "b2 = tf.random.normal(shape=[7,3,5])\n",
    "\n",
    "B = tf.einsum('bij,bjk -> bik', b1, b2) # Batch wise multiplication\n",
    "B.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c75eed46",
   "metadata": {},
   "source": [
    "c1 = tf.random.normal(shape=[7,5,6,4])\n",
    "c2 = tf.random.normal(shape=[7,5,4,6])\n",
    "\n",
    "B = tf.einsum('blij,bljk -> blik', c1,c2)\n",
    "B.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d18502",
   "metadata": {},
   "source": [
    "# Custom Huber Loss Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0517776b",
   "metadata": {},
   "source": [
    "Remember: To implement a custom loss function you have to simply implement two different methods:\n",
    "        1. def call(self, y_true, y_pred):  --> Returns the loss\n",
    "        2. get_config(self):  --> saves the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d2bbad8",
   "metadata": {},
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, threshold=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        condition = tf.abs(error) < self.threshold\n",
    "        return tf.where(condition, tf.square(error), tf.abs(error))\n",
    "        \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold':self.threshold}\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efb95f62",
   "metadata": {},
   "source": [
    "loss = HuberLoss(2.)\n",
    "#model.compile(loss = loss,...)\n",
    "\n",
    "\n",
    "# When u save the model the threshold will be saved along with it, when you reload the model, you just need to map the \n",
    "# class name to class itself.\n",
    "#model = tf.keras.models.load_model('model.h5',custom_objects={'HuberLoss':HuberLoss})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "095318f7",
   "metadata": {},
   "source": [
    "# Losses based on your Model Internals:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fb034",
   "metadata": {},
   "source": [
    "There will be times when you want to define losses based on other parts of your model, such as weights or acticvation of its\n",
    "hidden layers. To define acustom loss based on model_internals, compute it based on any part of the model you want, then pass\n",
    "the result to the add_loss() method. Lets define a reconstruction loss: Its mean squared difference between the recosntrcutions\n",
    "    and the inputs. By adding this reconstruction loss to main loss, we encourage the model to preserve as much as information\n",
    "    as possible through hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1d845",
   "metadata": {},
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # TODO: check https://github.com/tensorflow/tensorflow/issues/26260\n",
    "        #self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):        # We could have defined in constructor as well\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        #if training:\n",
    "        #    result = self.reconstruction_mean(recon_loss)\n",
    "        #    self.add_metric(result)\n",
    "        return self.out(Z)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e009d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70089a7",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "89f68236",
   "metadata": {},
   "source": [
    "# Custom Regualrizer, Initializer,Regularzer and Constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af6eb03c",
   "metadata": {},
   "source": [
    "weights = tf.random.normal([5,2])\n",
    "tf.zeros_like(weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35045fa3",
   "metadata": {},
   "source": [
    "# Step 1: Simply by function\n",
    "def softplus(value):\n",
    "    return tf.math.log(tf.math.exp(value) + 1.0)\n",
    "\n",
    "def glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.math.sqrt(2. / shape[0] + shape[1])\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def l1_regularizer(weights):\n",
    "    return tf.math.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def positive_weight_constraint(weights):  # You apply constraints on weights and just return the weights according to constraint\n",
    "    return tf.where(weights < 0, tf.zeros_like(weights), weights)\n",
    "\n",
    "\n",
    "layer = tf.keras.layers.Dense(100, activation = softplus, kernel_initializer = glorot_initializer, \n",
    "                             kernel_regularizer = l1_regularizer, kernel_constraint = positive_weight_constraint)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "22c9bbe3",
   "metadata": {},
   "source": [
    "# Note:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55b7de6e",
   "metadata": {},
   "source": [
    "1. The layers weights will be initialized using the value returned by the initializer. At each training step the\n",
    "weights will be passed to regularizer function to compute the regulariation loss, which will be added to main loss to get the final loss used for training. Finally the\n",
    "constraint function will be called after each training step, and the layer's weights will be replaced by the constrained\n",
    "weights.\n",
    "\n",
    "2. If you want to save some hyperparameters. You have to subclass the proper class like for regularizer it will be\n",
    "tf.keras.regularizers.Regularizer and so on and implment the get_config() method\n",
    "\n",
    "3. For losses, layers, activation functions, models  ----> must implement call() method\n",
    "   For regularizer, constraints, initializers  ------> must implement __call__() method\n",
    "   \n",
    "For metrics things are bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0d3bbcd",
   "metadata": {},
   "source": [
    "class L1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    \n",
    "    def __init__(self, factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, weights):\n",
    "        return tf.math.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'factor': self.factor}\n",
    "\n",
    "    \n",
    "layer = tf.keras.layers.Dense(100, activation = softplus, kernel_initializer = glorot_initializer, \n",
    "                             kernel_regularizer = L1Regularizer(0.01), kernel_constraint = positive_weight_constraint)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b764ec08",
   "metadata": {},
   "source": [
    "# Custom Metric:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf51fd",
   "metadata": {},
   "source": [
    "When you define a metric using a simple function, Keras automatically calls it for each batch,and it keeps track of the mean\n",
    "during each epoch, just like we did manually. So if you define the class for it the only advatage u get is get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a2a90dd",
   "metadata": {},
   "source": [
    "def create_huber(threshold):\n",
    "    def huber_metric(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        condition = tf.math.abs(error) < threshold\n",
    "        return tf.where(condition, tf.math.square(error), tf.math.abs(error))\n",
    "    return huber_metric"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e112d2d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def negative_predictive_value(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return tn / (tn + fn + K.epsilon())\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    num = (1 + beta ** 2) * (p * r)\n",
    "    den = (beta ** 2 * p + r + K.epsilon())\n",
    "    return K.mean(num / den)\n",
    "\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())\n",
    "\n",
    "\n",
    "def equal_error_rate(y_true, y_pred):\n",
    "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
    "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
    "\n",
    "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
    "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
    "\n",
    "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
    "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
    "    body = lambda t, fpr, fnr: (\n",
    "        t + 0.001,\n",
    "        tf.divide(tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
    "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
    "    )\n",
    "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
    "    eer = (fpr + fnr) / 2\n",
    "\n",
    "    return eer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc7ea616",
   "metadata": {},
   "source": [
    "model.compile(loss = 'mse', optimizer='nadam', metrics = [create_huber(2.0)]) "
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "b569f75b",
   "metadata": {},
   "source": [
    "While implementing it through class its bit different. You need to implement two methods:\n",
    "    1. def update_state(self, y_true, y_pred, sample_weight=None)   ----> Used to update weights and variables\n",
    "    2. def result(self)  ----> returns the metric score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea9249e0",
   "metadata": {},
   "source": [
    "example = tf.reshape(tf.range(100),[25,4])\n",
    "print(tf.size(example))\n",
    "print(tf.shape(example))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53961314",
   "metadata": {},
   "source": [
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.total = self.add_weight('total', initializer = 'zeros')\n",
    "        self.count = self.add_weight('count', initializer = 'zeros')\n",
    "    \n",
    "    @staticmethod\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        condition = tf.math.abs(error) < threshold\n",
    "        return tf.where(condition, tf.math.square(error), tf.math.abs(error))\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.math.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold':self.threshold}"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "4bcd2866",
   "metadata": {},
   "source": [
    "1. Constructor uses add_weight() method to create variables needed to keep track of the metrics state over multiple batches.\n",
    "In this case sum of all huberloss. You can also create variable manually NO PROBLEM. Its just keras keep track of all the \n",
    "vairables.\n",
    "\n",
    "2. update_State() is called when you use an instance of this class as a function. It updates the variables, given the labels\n",
    "and prediction for ONE BATCH AND SAMPLE_WEIGHTS WHICH WE IGNORED HERE\n",
    "\n",
    "3. The result() computes and returns the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b9b8b",
   "metadata": {},
   "source": [
    "# Custom layers and Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d54a6f",
   "metadata": {},
   "source": [
    "There are two ways of doing it. Eiher you subclass layer class or model class.\n",
    "The only methods you need to implement are:\n",
    "    \n",
    "    1. def __init__(self): Here just initialize the constant parameters. You can also intialize weights here but its not\n",
    "        recommended to do so. For initializing the weigths use the build method.\n",
    "    \n",
    "    2. def build(self, inputs_shape): Initialize your variables here.\n",
    "    \n",
    "    3. def call(self, inputs): It does the computation here on the inputs\n",
    "        \n",
    "In cases where you know the input_dimension in advance you can initialize them in constructor and in that case you need not\n",
    "to implement build() function. But if you dont know always use build() it will give u the size."
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "034ea187",
   "metadata": {},
   "source": [
    "First, some layers have no weights, such as keras.layers.Flatten or keras.lay\n",
    "ers.ReLU. If you want to create a custom layer without any weights, the simplest\n",
    "option is to write a function and wrap it in a keras.layers.Lambda layer. For example,\n",
    "the following layer will apply the exponential function to its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5656c0a2",
   "metadata": {},
   "source": [
    "custom_layer = tf.keras.layers.Lambda(lambda x:tf.math.exp(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d56443ab",
   "metadata": {},
   "source": [
    "X = tf.range(10, dtype=tf.float32)\n",
    "custom_layer(X)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f6aab26",
   "metadata": {},
   "source": [
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight('Kernel',\n",
    "                                     shape=(input_shape[-1], self.units),\n",
    "                                     initializer=tf.keras.initializers.he_normal,\n",
    "                                     dtype = 'float',\n",
    "                                     trainable=True)        # These weights will be tuned by Backpropagation\n",
    "        \n",
    "        self.bias = self.add_weight('Bias', shape = (self.units,), initializer=tf.keras.initializers.zeros, dtype = 'float',\n",
    "                                   trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel) + self.bias\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units':self.units}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0260312",
   "metadata": {},
   "source": [
    "# At instantiation, we don't know on what inputs this is going to get called\n",
    "layer = DenseLayer(32)\n",
    "X = tf.reshape(tf.range(10,dtype='float'),[5,2])\n",
    "\n",
    "# The layer's weights are created dynamically the first time the layer is called\n",
    "out = layer(X)  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "11492851",
   "metadata": {},
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out = out_units\n",
    "        self.layer1 = DenseLayer(32)\n",
    "        self.layer2 = DenseLayer(32)\n",
    "        self.layer3 = DenseLayer(self.out)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        layer1 = tf.nn.relu(self.layer1(inputs))\n",
    "        layer2 = tf.nn.relu(self.layer2(layer1))\n",
    "        layer3 = tf.nn.softmax(self.layer3(layer2))\n",
    "        return layer3\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f33dc3ea",
   "metadata": {},
   "source": [
    "mlp = MLP(10)\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))\n",
    "y # shape of y will be (batch,outputdim)"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "b8f3c53a",
   "metadata": {},
   "source": [
    "Many interesting layer-like things in machine learning models are implemented by composing existing layers. \n",
    "For example, each residual block in a resnet is a composition of convolutions, batch normalizations, and a shortcut. \n",
    "Layers can be nested inside other layers.\n",
    "\n",
    "Typically you inherit from keras.Model when you need the model methods like: Model.fit,Model.evaluate, and Model.save \n",
    "(see Custom Keras layers and models for details).\n",
    "\n",
    "One other feature provided by keras.Model (instead of keras.layers.Layer) is that in addition to tracking variables, \n",
    "a keras.Model also tracks its internal layers, making them easier to inspect.\n",
    "\n",
    "One Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "581e7fb0",
   "metadata": {},
   "source": [
    "# We could have also subclassed tf.keras.layers.Layer\n",
    "\n",
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9c526f0",
   "metadata": {},
   "source": [
    "_ = block(tf.zeros([1, 2, 3, 3]))       # This will execute the call() method"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4647f867",
   "metadata": {},
   "source": [
    "block.layers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68cede17",
   "metadata": {},
   "source": [
    "# Part 19\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_neurons,n_layers,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.neurons = n_neurons\n",
    "        self.layers = n_layers\n",
    "\n",
    "        self.hidden = [tf.keras.layers.Dense(self.neurons,activation='elu',kernel_initializer='he_normal')\n",
    "                      for _ in range(self.layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        for layer in self.hidden:\n",
    "            X = layer(X)\n",
    "        return X + inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d83153c2",
   "metadata": {},
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,neurons = 3,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.neurons = neurons\n",
    "        self.residualB = ResidualBlock(n_neurons = self.neurons, n_layers = 2)\n",
    "        self.residualA = [ResidualBlock(self.neurons,2) for _ in range(4)]\n",
    "        self.denseA = tf.keras.layers.Dense(self.neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.denseB = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.denseA(inputs)\n",
    "        for layer in self.residualA:\n",
    "            Z = layer(Z)\n",
    "        Z = self.residualB(Z)\n",
    "        return self.denseB(Z)\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8ed3559",
   "metadata": {},
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48fd8671",
   "metadata": {},
   "source": [
    "model = ResidualRegressor(neurons = 30)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12b813c2",
   "metadata": {},
   "source": [
    "model.compile(loss=loss, optimizer='nadam') #loss = Huberloss(2.)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "878db262",
   "metadata": {},
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_valid,y_valid), epochs=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d0b9a01",
   "metadata": {},
   "source": [
    "# Gradient Tape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c76826",
   "metadata": {},
   "source": [
    "Note: To calulate gradient you dont have to go and actually calculate its gradient you can do approximation For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44da0e2c",
   "metadata": {},
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c102cbdc",
   "metadata": {},
   "source": [
    "# What will be gradient at point (5, 3)\n",
    "\n",
    "# One way:\n",
    "w1, w2 = 5, 3\n",
    "gradient_w1, gradient_w2 = 6*w1 + 2*w2, 2*w1\n",
    "print(gradient_w1, gradient_w2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86e5df31",
   "metadata": {},
   "source": [
    "# Another way nice to do for small functions. As u can see we need to call f() atleast once per parameter. So intractable for\n",
    "# Neural Network :P\n",
    "eps = 1e-6\n",
    "\n",
    "# With respect to w1\n",
    "print('w.r.t w1: ',round((f(w1+eps,w2) - f(w1,w2))/eps, 2))\n",
    "\n",
    "\n",
    "# With respect to w2\n",
    "print('w.r.t w2: ',round((f(w1,w2+eps) - f(w1,w2))/eps, 2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b1c321b",
   "metadata": {},
   "source": [
    "# Lets use gradienttape which will automatically record every operation that involves a variable\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "# And finally we ask this tape to calculate the gradient of result z w.r.t the variables [w1, w2]\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cfa46d7",
   "metadata": {},
   "source": [
    "# The tape gets erased after you call the gradient once so if u try to call again gradient you will get an error\n",
    "tape.gradient(z , w1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1f05e1c",
   "metadata": {},
   "source": [
    "# To make it persistant make\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1,w2)\n",
    "\n",
    "print(tape.gradient(z, w1))\n",
    "print(tape.gradient(z ,w2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a7d4a05",
   "metadata": {},
   "source": [
    "# Note the w1, and w2 must be Variables not Constant if they are and you try to calculate the gradient on them. You will get \n",
    "# [None, None]. To calulate gradient on constants c1, and c2 you need to watch them within Gradienttape.\n",
    "\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1,c2)\n",
    "    \n",
    "print('w.r.t c1', tape.gradient(z,c1))\n",
    "print('w.r.t c2', tape.gradient(z,c2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ba385a9",
   "metadata": {},
   "source": [
    "# Gradient tape just do one forward pass and one backward pass to compute all the gradients. If you want to stop some part \n",
    "# so that it doesnt flow the gradient, use tf.stop_gradient() on it. It will return Identity during forward pass but during \n",
    "# backward pass it wont let the gradient flow\n",
    "\n",
    "def f(w1 , w2):\n",
    "    return 3 * w1**2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "print(tape.gradient(z,[w1,w2]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28b4aa45",
   "metadata": {},
   "source": [
    "# Calculate Jacobian and Hessian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5966512a",
   "metadata": {},
   "source": [
    "def f(w1, w2, w3):\n",
    "    return 2*w1**2 + 3*w1*w2*w3 + 2*w2 + 3*w3**0.5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "897a4162",
   "metadata": {},
   "source": [
    "w1, w2, w3 = tf.Variable(1.), tf.Variable(2.), tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as hessian:\n",
    "    with tf.GradientTape(persistent=False) as jacobian:\n",
    "        z = f(w1,w2,w3)\n",
    "        jacobian = jacobian.gradient(z, [w1, w2, w3])\n",
    "\n",
    "hessians = [hessian.gradient(jacob,[w1, w2, w3]) for jacob in jacobian]\n",
    "hessians"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3170c3a4",
   "metadata": {},
   "source": [
    "jacobian"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f6264876",
   "metadata": {},
   "source": [
    "# Custom Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21ef3846",
   "metadata": {},
   "source": [
    "# Lets implement on mnist dataset\n",
    "\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "(x_train,y_train),(x_valid, y_valid) = dataset.load_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cd01a5d",
   "metadata": {},
   "source": [
    "x_train = tf.image.per_image_standardization(x_train)\n",
    "x_valid = tf.image.per_image_standardization(x_valid)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de203def",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=x_train.shape[1:]),  \n",
    "    # if u dont pass flatten to give input size then weights will be defined at fit time i.e. build() will be executed\n",
    "    tf.keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1e0592a",
   "metadata": {},
   "source": [
    "model.layers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cda62922",
   "metadata": {},
   "source": [
    "model.weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c196bc1e",
   "metadata": {},
   "source": [
    "def random_batch(X,Y,batch_size=32):\n",
    "    idx = np.random.randint(len(X),size=batch_size)\n",
    "    return X[idx], Y[idx]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b13ca324",
   "metadata": {},
   "source": [
    "epochs = 10\n",
    "steps = x_train.shape[0] // batch_size   \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd296d0e",
   "metadata": {},
   "source": [
    "for epoch in range(epochs):\n",
    "    for step in range(steps):\n",
    "        X,Y = random_batch(x_train.numpy(),y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X, training=True)\n",
    "            losses = loss(Y, y_pred)\n",
    "        gradient = tape.gradient(losses, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(losses))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "99c2204d",
   "metadata": {},
   "source": [
    "Lets include the SparseCategorical Accuracy as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c07ef3e2",
   "metadata": {},
   "source": [
    "batch_size = 32\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "train_metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "val_steps = x_valid.shape[0] // batch_size"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4580d90",
   "metadata": {},
   "source": [
    "for epoch in range(epochs):\n",
    "    for step in range(steps):\n",
    "        X,Y = random_batch(x_train.numpy(), y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X, training=True)\n",
    "            loss_function = loss(Y,y_pred)\n",
    "        gradients = tape.gradient(loss_function, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        # Update the training metrics\n",
    "        train_metrics.update_state(y_pred, Y)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_function))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "    # At the end of each epoch return the metric score\n",
    "    train_score = train_metrics.result()\n",
    "    print('Train Accuracy at epoch {} is {}'.format(epoch, round(train_score,2)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    \n",
    "    # Lets calcualte the validation score\n",
    "    \n",
    "    for val_step in range(val_steps):\n",
    "        x_valid,y_valid = random_batch(x_valid.numpy(),y_valid)\n",
    "        y_pred = model(x_valid,training=False)\n",
    "        \n",
    "        val_metrics.update_state(y_pred,y_valid)\n",
    "    \n",
    "    # At the end of each epoch return the metric score\n",
    "    val_score = val_metrics.result()\n",
    "    print('Val Accuracy at epoch {} is {}'.format(epoch, round(val_score,2)))\n",
    "    \n",
    "    # Reset validation metrics at the end of each epoch\n",
    "    val_metrics.reset_states()\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1f97165c",
   "metadata": {},
   "source": [
    "# Callbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906ba7c",
   "metadata": {},
   "source": [
    "Exponential Scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a265591",
   "metadata": {},
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X_train,X_test,Y_train,Y_valid = train_test_split(data.data, data.target, train_size=0.8, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "106ad2ab",
   "metadata": {},
   "source": [
    "x_train,x_valid,y_train,y_valid = train_test_split(X_train, Y_train, train_size=0.8, random_state=12)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53ca1014",
   "metadata": {},
   "source": [
    "scalar = StandardScaler()\n",
    "x_train = scalar.fit_transform(x_train)\n",
    "x_valid = scalar.transform(x_valid)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52c3d3d5",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(50, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4839969e",
   "metadata": {},
   "source": [
    "model.compile(loss=tf.keras.losses.mean_squared_error, optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95225d",
   "metadata": {},
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d335711",
   "metadata": {},
   "source": [
    "K = tf.keras.backend\n",
    "class ExponentialSchedule(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, s = 100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.s = s\n",
    "        self.lr = []\n",
    "        self.loss = []\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.lr.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.loss.append(K.get_value(logs['loss']))\n",
    "        K.set_value(self.model.optimizer.lr, 0.01**(1/self.s) * self.model.optimizer.lr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d0c93c8",
   "metadata": {},
   "source": [
    "epochs = 20\n",
    "s = epochs * len(x_train) // 32    # s represents total number of steps\n",
    "\n",
    "callback = ExponentialSchedule(s)\n",
    "model.fit(x_train,y_train, validation_data=(x_valid, y_valid), epochs=epochs,callbacks=callback)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0102af4",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(callback.lr,callback.loss,color = 'red')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('loss')\n",
    "plt.grid('True')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "115949b5",
   "metadata": {},
   "source": [
    "# Data Loader using Keras Sequence and Tensorflow Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214ab81",
   "metadata": {},
   "source": [
    "# Lets solve Dogs Vs Cat dataset problem we will implement the dataloader as well as the tensorflow data api."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e69633",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40619814",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d73fe",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5ed8b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b242b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "72fc70ef",
   "metadata": {},
   "source": [
    "# CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b5e1ae77",
   "metadata": {},
   "source": [
    "dataset = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test) = dataset.load_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9752c35b",
   "metadata": {},
   "source": [
    "X_train.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d24a7ac1",
   "metadata": {},
   "source": [
    "n_rows = 5\n",
    "n_cols = 5\n",
    "plt.figure(figsize=(10,10))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols*row + col\n",
    "        plt.subplot(n_rows,n_cols,index+1)\n",
    "        plt.imshow(X_train[index], cmap = 'binary', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(Y_train[index],fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.3,hspace=0.4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16a6ff2b",
   "metadata": {},
   "source": [
    "X_train = tf.image.per_image_standardization(X_train)\n",
    "train_size = int(0.8 * len(X_train))\n",
    "x_train,y_train = X_train[:train_size],Y_train[:train_size]\n",
    "x_valid,y_valid = X_train[train_size:],Y_train[train_size:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "af50124f",
   "metadata": {},
   "source": [
    "# Conv2d deals with image having three dimension (width,height,channels). Currently we have two\n",
    "\n",
    "x_train = x_train[...,tf.newaxis]\n",
    "x_valid = x_valid[...,tf.newaxis]\n",
    "X_test = tf.image.per_image_standardization(X_test)\n",
    "X_test = X_test[...,tf.newaxis]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30af0e60",
   "metadata": {},
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv = partial(tf.keras.layers.Conv2D,kernel_size = 3, strides = 1,padding='SAME',activation='relu')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    DefaultConv(filters=64, kernel_size = 7, input_shape=[28,28,1]),  # Input shape RGB\n",
    "    tf.keras.layers.MaxPool2D(pool_size = 2),\n",
    "    DefaultConv(filters = 128),\n",
    "    DefaultConv(filters = 128),\n",
    "    tf.keras.layers.MaxPool2D(pool_size = 2),\n",
    "    DefaultConv(filters = 256),\n",
    "    DefaultConv(filters = 256),\n",
    "    tf.keras.layers.MaxPool2D(pool_size = 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5756c829",
   "metadata": {},
   "source": [
    "Note: Maxpooling layers doesnt have any weights associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9704b13c",
   "metadata": {},
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eb4b90ea",
   "metadata": {},
   "source": [
    "model.trainable_variables"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5393bcc3",
   "metadata": {},
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "                                                                                                      metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1158700c",
   "metadata": {},
   "source": [
    "history = model.fit(x_train,y_train,epochs=1,validation_data=(x_valid,y_valid))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ecf4296f",
   "metadata": {},
   "source": [
    "model.evaluate(X_test,Y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c7b87e0a",
   "metadata": {},
   "source": [
    "tf.argmax(model.predict(X_test),axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "788a1a7d",
   "metadata": {},
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2090b710",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size= 7,strides=2, input_shape=[224,224,3]))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size = 3,strides = 2, padding='SAME'))\n",
    "\n",
    "prev_filter = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    stride = 1 if prev_filter == filters else 2\n",
    "    model.add(ResidualBlock(filters = filters,stride = stride))\n",
    "    prev_filter = filters\n",
    "    \n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18953f9",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e213a42",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b49d67e",
   "metadata": {},
   "source": [
    "# Part 2: Machine Learning and Sklearn, Pandas, Numpy, Matplotlib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c0eda",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_deepai",
   "language": "python",
   "name": "tf2_deepai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
